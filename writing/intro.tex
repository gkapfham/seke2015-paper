\section{Introduction}

Search-based algorithms allow the application of guidance to problems.
The algorithm attempts to improve a potential solution until the
solution is acceptable. Without the use of a search-based strategy, a
problem might be approached with a random sampling or greedy
technique. In the
domain of data generation for software testing, this means that rather
than randomly selecting inputs from a program's input space, the data
generator can actively seek out qualities of an input that best fulfill
the test's goals \cite{McMinn2004a}. While this
technique has been applied to various problems, including test suite
prioritization \cite{Walcott:tsp} and testing
relational database schemas \cite{Kapfhammer2013}, 
as far as we know, no research has been done on evaluating the efficiency
of search-based test data generation. 

Because these search-based systems are complex, they are difficult to
analyze theoretically. To overcome this challenge, we attack
search-based test data generation with an empirical approach. This paper presents an empirical study of the search-based data
generation tool, called \textit{SchemaAnalyst}, which generates test suites for
relational database schemas.  To evaluate \textit{SchemaAnalyst}, we systematically double the size of the
program's input schema and record the change in its execution time.
Using this process, we explore possible configurations of
\textit{SchemaAnalyst}, revealing trade-offs in the performance of
search-based test data generation with respect to the test's goals, the
structure of the input schema, and the data generation strategy.

\begin{enumerate}
  \item A framework for automated doubling experiments
    (Section~\ref{subsec:doubling}).
  \item An empirical study evaluating the efficiency of a search-based
    data generation tool (Section~\ref{subsec:experiment}).
  \item A discussion of the trade-offs between the parameters of
    search-based test data generation and performance.  
    (Section~\ref{sec:results})

  \end{enumerate}
