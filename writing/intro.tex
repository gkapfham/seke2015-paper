\section{Introduction}

Many disciplines such as science, finance, and medicine,
rely on relational databases to maintain large amounts
of critical information. The relational database schema
defines the structure of a database and protects the
consistency of the data. This makes testing the database
schema necessary to avoid the corruption of data. Search-based
algorithms have been applied to this problem~\cite{Kapfhammer2013}, and
others such as test suite prioritization~\cite{Walcott:tsp}.

Search-based algorithms allow the application of guid-
ance to problems. The algorithm attempts to improve
a potential solution until it is acceptable according to
a fitness function. Without the use of a search-based
strategy, a problem might be approached with a random
sampling or greedy technique. In the domain of data
generation for software testing, this means that rather
than randomly selecting inputs from a program’s input
space, the data generator can actively seek out qualities
of an input that best fulfills the test’s goals~\cite{McMinn2004a}.

Despite the effectiveness of search-based techniques,
there is no research regarding their efficiency reported
in the literature. Because these search-based systems
are complex, they are often difficult to analyze theoret-
ically. To overcome this challenge, we attack the per-
formance evaluation of search-based test data generation
with an empirical approach. We present an empirical performance
study of the search-based test data generation tool, called
\textit{SchemaAnalyst}, which generates test suites for relational
database schemas. To evaluate worst-case performance,
we developed a technique for automatically conducting
doubling experiments. Using this process, we explored \textsc{xmany} 
configurations of \textit{SchemaAnalyst}, revealing trade-offs
in the performance of search-based test data generation
with respect to the test’s goals, the structure of the
input schema, and the data generation strategy. Our
automatic technique enabled a comprehensive empirical
study, that would otherwise have been infeasible, and the results
have important practical significance for the selection of
parameters in search-based test data generation tools.

\begin{enumerate}
  \item A framework for automated doubling experiments
    (Section~\ref{subsec:doubling}).
  \item An empirical study evaluating the efficiency of a search-based
    data generation tool (Section~\ref{subsec:experiment}).
  \item A discussion of the trade-offs between the parameters of
    search-based test data generation and performance.  
    (Section~\ref{sec:results})
  \end{enumerate}
