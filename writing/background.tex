%!TEX root=seke.tex
% mainfile: ../seke.tex

\section{Background and Related Work}

% Do we need to talk about databases/schemas/constraints?
% if so it should go here, and we're going to have major space issues

%TODO GMK: discuss database related content
%TODO GMK: Discuss coverage criterion
%TODO GMK: Explain the subsumption hierarchy
%TODO GMK: Discuss data generators

% \subsection{Search-Based Test Data Generation}

{\bf Testing Database Schemas.} The relational database, a cornerstone of modern software, is protected by a schema that
defines integrity constraints ensuring the coherence of data. These constraints defend the schema from manipulations
that could violate requirements such as ``user names must be unique'' or ``the host name cannot be missing or unknown''.
Prior work in this area proposed coverage criteria, derived from logic coverage criteria, that establish different
levels of testing for the formulation of integrity constraints in a database schema~\cite{mcminn2015}. These range from
simple criteria that mandate the testing of successful and unsuccessful INSERT statements into tables to more advanced
criteria that test the formulation of complex integrity constraints such as multi-column PRIMARY KEYs and arbitrary
CHECK constraints. This family of criteria has been organized into a subsumption hierarchy, with criterion such as {\em
Clause-Based Active Integrity Constraint Coverage} (ClauseAICC) emerging as a stringent testing strategy. Since space
constraints limit further commentary on testing methods for database schemas, the reader is referred
to~\cite{mcminn2015} for more details.

% Since the quality of the test depends upon the characteristics of the data itself, test data generators systematically
% produce test inputs according to a criterion. 

{\bf Search-Based Test Data Generation}. When testing a schema's integrity constraints for correctness, it is often
necessary to provide input to the database and observe and evaluate its execution~\cite{kapfhammer2013}.  Since the
database's behavior is dependant on the input from INSERTs, the input space must be sufficiently explored to ensure
thorough testing.  Due to the fact that it is challenging to manually create input that supports high-quality testing,
test data generation is used to automatically produce it according to a criterion, like ClauseAICC. A search-based test
data generator is one that explores that input space using, among other components, a fitness function that rates the
data's quality, thus allowing it to improve by repeatedly searching for better inputs~\cite{mcminn2004a}.

{\bf Worst-Case Time Complexity}. Worst-case time complexity is a useful measure of an algorithm's efficiency, or how
increasing the size of the input $n$ increases the execution time of the algorithm, $f(n)$.  Worst-case refers to the
efficiency in the worst possible scenario.  This relationship is often expressed in big-Oh notation, where $f(n)$ is
$O(g(n))$ means that the time increases by no more than on order of $g(n)$. The worst-case complexity of an algorithm is
evident when $n$ is large~\cite{Goodrich2014}. One approach for determining the big-Oh complexity of an algorithm is to
conduct a doubling experiment. By measuring the time needed to run the algorithm on an input $n$, and the time needed to
run on $2n$, the order of growth of the algorithm can be determined~\cite{McGeoch2012,Sedgewick1998}.

Intuitively, the goal of a doubling experiment is to draw a conclusion regarding the efficiency of the algorithm from
the ratio $f(2n)/f(n)$. This ratio represents the factor of change in runtime from input $n$ to $2n$. A ratio of $2$
would indicate that doubling the input resulted in runtime doubling. We could then conclude that the algorithm under
study is $O(n)$ or $O(n\log n)$.  Table~\ref{table:ratios} shows some common time complexities and the corresponding
ratios.

{\bf Related Work}. Goldsmith et al.~\cite{Goldsmith2007} developed a system to empirically evaluate computational
complexly.  Their system, \textit{Trend-Prof}, uses code instrumentation to count the number of times each block of code
is executed, and then groups these blocks by their behavior.  \textit{Trend-Prof} takes in a collection of workloads,
user specified features of the workloads, and the program to be studied. This technique results in a more powerful
analysis. However, the authors do not address the issue of generating the workloads necessary to achieve a meaningful
result, and we attempt to do this automatically.  Additionally, our approach is novel because we apply it to a domain
where the theoretical scalability is not yet known.

\begin{table}[t]

  \begin{center}
    \begin{tabular}{l|l}
      Ratio & Conclusion              \\ \hline
      1     & Constant or Logarithmic \\
      2     & Linear or Linearithmic  \\
      4     & Quadratic               \\
      8     & Cubic                   \\
      x     & $O(n^{\log x})$
    \end{tabular}
  \end{center}
  \vspace*{-.15in}

  \caption{Conclusions regarding efficiency that can be drawn from the doubling ratio.}\label{table:ratios}
  \vspace*{-.30in}

\end{table}
