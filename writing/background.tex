%!TEX root=seke.tex
% mainfile: ../seke.tex

\vspace*{-.05in}
\section{Background and Related Work}
\vspace*{-.05in}

% Do we need to talk about databases/schemas/constraints?
% if so it should go here, and we're going to have major space issues

%TODO GMK: discuss database related content
%TODO GMK: Discuss coverage criterion
%TODO GMK: Explain the subsumption hierarchy
%TODO GMK: Discuss data generators

% \subsection{Search-Based Test Data Generation}

{\bf Testing Database Schemas.} The relational database, a cornerstone of modern software, is protected by a schema that
defines integrity constraints ensuring the coherence of data. These constraints defend the schema from manipulations
that could violate requirements such as ``user names must be unique'' or ``the host name cannot be missing or unknown''.
Prior work in this area proposed coverage criteria, derived from logic coverage criteria, that establish different
levels of testing for the formulation of integrity constraints in a database schema~\cite{mcminn2015}. These range from
simple criteria that mandate the testing of successful and unsuccessful {\tt INSERT} statements into tables to more
advanced criteria that test the formulation of complex integrity constraints such as multi-column {\tt PRIMARY~KEY}s and
arbitrary {\tt CHECK} constraints. This family of criteria has been organized into a subsumption hierarchy, with
criteria such as {\em Clause-Based Active Integrity Constraint Coverage} (ClauseAICC) emerging as a stringent testing
strategy. Since space constraints limit further commentary on testing methods for database schemas, the reader is
referred to~\cite{mcminn2015} for more details.

% Since the quality of the test depends upon the characteristics of the data itself, test data generators systematically
% produce test inputs according to a criterion.

{\bf Search-Based Test Data Generation}. When testing a schema's integrity constraints for correctness, it is often
necessary to provide input to the database and observe and evaluate its execution~\cite{kapfhammer2013}.  Since the
database's behavior is dependant on the input from {\tt INSERT}s, the input space must be sufficiently explored to ensure
thorough testing.  Due to the fact that it is challenging to manually create input that supports high-quality testing,
test data generation is used to automatically produce it according to a criterion, like ClauseAICC. A search-based test
data generator is one that explores that input space using, among other components, a fitness function that rates the
data's quality, thus allowing it to improve by repeatedly searching for better inputs~\cite{mcminn2004a}.

{\bf Worst-Case Time Complexity}. A useful understanding of an algorithm's efficiency, the worst-case time complexity
gives an upper bound on how an increase in the size of the input, denoted $n$, increases the execution time of the
algorithm, $f(n)$.  This relationship is often expressed in the ``big-Oh'' notation, where $f(n)$ is $O(g(n))$ means
that the time increases by no more than on order of $g(n)$. Since the worst-case complexity of an algorithm is evident
when $n$ is large~\cite{goodrich2014}, one approach for determining the big-Oh complexity of an algorithm is to conduct
a doubling experiment with increasingly bigger input sizes. By measuring the time needed to run the algorithm on an
input of size $n$ and the time needed to run with input of size $2n$, the algorithm's order of growth can be
empirically determined~\cite{mcgeoch2012,sedgewick1998}.

The goal of a doubling experiment is to draw a conclusion regarding the efficiency of the algorithm from the ratio
$f(2n)/f(n)$ that represents the factor of change in runtime from input sizes $n$ to $2n$. For instance, a ratio of $2$
would indicate that doubling the input size resulted in the runtime's doubling, thus leading to the conclusion that the
algorithm under study is $O(n)$ or $O(n\log n)$.  Table~\ref{table:ratios} shows some common time complexities and their
corresponding ratios.

\begin{table}[t]

  \begin{center}

    \begin{tabular}{c|l}
      Ratio $f(2n)/f(n)$ & Worst-Case Conclusion              \\ \hline
      1                  & constant or logarithmic \\
      2                  & linear or linearithmic  \\
      4                  & quadratic               \\
      8                  & cubic                   \\
      % x                & $O(n^{\log x})$
    \end{tabular}

  \end{center}
  \vspace*{-.25in}

  % \caption{Worst-case time complexity conclusions that can be drawn from the doubling ratio $f(2n)/f(n)$.}\label{table:ratios}
  \caption{Conclusions for worst-case time complexity.} 
  \vspace*{-.25in}

\end{table}

{\bf Related Work}. Goldsmith et al.~\cite{Goldsmith2007} developed a tool, called \textit{Trend-Prof}, that empirically
evaluates the computational complexity of a program by using code instrumentation to count the number of times each
block of code is executed and then grouping these blocks by their behavior.  \textit{Trend-Prof} takes in a collection
of workloads, user-specified features of the workloads, and the program to be studied. While this technique results in a
more detailed analysis that the one presented in this paper, Goldsmith et al.\ did not address the issue of generating
the workloads necessary to achieve a meaningful result, which this paper's technique can handle automatically.  Our
paper is also contrasted with this prior work because it describes experiments in a domain, search-based test data
generation, where the method's worst-case time complexity is not always known.

Zhao et al.\ presented an empirical study of the performance of search-based test data generation for extended finite
state machine (EFSM) models~\cite{zhao2010}. Although this paper focused on efficiency and made preliminary observations
about the relationship between performance and the characteristics of an EFSM model, it did not, like our paper, use
doubling experiments to suggest worst-case time complexities.  Lakhotia et al.\ also reported on an experimental
analysis of the efficiency and effectiveness of search-based test data generation for C programs~\cite{lakhotia2013}.
While our paper looks at generator performance in a holistic manner, this prior work considered the number of fitness
evaluations during data generation. Similar to our use of doublers that systematically increase the size of a relational
schema, Mehrmand and Feldt empirically studied search-based data generation as the size of the program
increases~\cite{mehrmand2010}. Yet, their focus is on the generator's success in covering source code branches instead
of the efficiency of the data generation process.

% GMK NOTE: Adding in the references to the two papers suggested by PM

The empirical work presented in this paper is complemented by theoretical runtime analyses in prior research.  For
instance, Arcuri presented the first runtime analysis of a search-based test data generator called the alternating
variable method (AVM) \cite{arcuri2009}, which is also studied in this paper. Arcuri proves the worst-case time
complexity of AVM when it generates data for a simple program called ``triangle classification''. More recently, Kempka
et al.\ extended the work of Arcuri with a theoretical and empirical runtime analysis revealing that the use of certain
local search techniques with AVM yields better performance than AVM alone \cite{kempka2015}.  While our paper's
automated framework can easily be applied to new schemas---and even other types of search-based test data
generators---the results in these two aforementioned papers are more difficult to generalize.

% GMK NOTE: I think that I will not cite the Yoo et al. paper right now, due to lack of space.

% Finally, in an acknowledgement of the poor
% performance of many search-based methods, Yoo et al. explain
