\section{Background}

\subsection{Search-Based Test Data Generation}
When testing a system for correctness, it is often necessary to
provide input to the system in order to observe and evaluate its
execution. Additionally, when the system's behavior is dependant on the
input, then the input space must be sufficiently explored to ensure a
thorough test.  Obtaining enough input data for a high quality test can
be challenging, so test data generation is used to automatically
produce this data. Since the quality of the test depends upon the
quality of the data, test data generation tools produce data according
to some requirement or criterion. A search based test data generation
tool is one that explores that data space using a fitness function.  The
fitness function rates the quality of the data, and allows the generator
to try and improve by searching for higher quality.

\subsection{Worst Case Time Complexity}

Worst-case time complexity is a useful measure of an algorithms
efficiency, or how increasing the size
of the input $n$ increases the execution time of the algorithm, $f(n)$.
Worst-case refers to the efficiency in the worst possible scenario. This
means that worst-case time complexity tries to make a guarantee that
there is no way that the runtime will be worse than this, although it
may, perhaps even nearly always, be faster. 
This relationship is often expressed in big-Oh notation, where $f(n)$
is $O(g(n))$ means that the time increases by no more than on order of $g(n)$. The
worst-case complexity of an algorithm is evident when $n$ is large 
\cite{Goodrich:Data}. One approach for determining the big-Oh complexity
of an algorithm is to conduct a doubling experiment. By measuring the
time needed to run the algorithm on an input $n$, and the time needed to
run on $2n$, the order of growth of the algorithm can be determined \cite{Mcgeoch:Algorithmics,Sedgewick:Analysis}. 

\subsection{Doubling Experiment}

Intuitively, the goal of a doubling experiment is to draw a conclusion
regarding the efficiency of the algorithm from the ratio
$f(2n)/f(n)$. This ratio represents the factor of change in runtime from
input $n$ to $2n$. A ratio of $2$ would indicate that doubling the
input resulted in runtime doubling. We could then conclude that the
algorithm under study is $O(n)$ or $O(n\log n)$.
Table~\ref{table:ratios} shows some common time complexities and the
corresponding ratios.

\begin{table}[h]
\begin{tabular}{l|l}
Ratio & Conclusion              \\ \hline
1     & Constant or Logarithmic \\
2     & Linear or Linearithmic  \\
4     & Quadratic               \\
8     & Cubic                   \\
x     & $O(n^{\log x})$          
\end{tabular}
\label{table:ratios}
\caption{Conclusions regarding efficiency that can be drawn from the
doubling ratio.}
\end{table}
