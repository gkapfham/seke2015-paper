%!TEX root=seke.tex
% mainfile: ../seke.tex

\vspace*{-.035in}
\section{Automated Doubling Experiments}\label{sec:technique}
\vspace*{-.035in}

  \begin{figure*}
    \input{writing/diagram}
    \vspace*{-.1in}
    \caption{Technique for conducting automatic doubling experiments.}~\label{fig:doublingexp}
    \vspace*{-.1in}
  \end{figure*}

  % GMK NOTE: We need to make sure that at some point we also talk about the automated analysis of the results.

  {\bf Overview}. The presented technique for performing automatic doubling experiments consists of two key components.
  The first is a method for systematically doubling the initially input relational schema, and the second is a rule for
  determining when a conclusion can be drawn from the experiment, thus allowing the experiment to stop.

  \textbf{Doubling Schemas}. Determining worst-case complexity by a doubling experiment requires that the size of the
  input be doubled. A relational database schema is a complex artifact with many features and interrelationships.  This
  makes the doubling rule implementation a non-trivial task.

  % GMK NOTE: This paragraph was too longer for the reader to easily understand. Breaking it up!

  A relational database schema contains tables and columns, and constraints that restrict the values allowed into the
  tables. Since the runtime of a schema testing tool may be affected by the number of any of these, it is best to have a
  strategy for doubling each component of a schema. Doubling the number of tables or columns in a schema is relatively
  easy.  We can double the number of tables in a schema by following the rule: for every table present in the schema, we
  create a new empty table. It is important that the new tables be empty to avoid changing multiple variables at once.
  If the new tables contained columns, for instance, then the number of tables and columns in the schema would be
  increased, interfering with our ability to measure the effect of tables alone.  Doubling the number of columns can be
  accomplished by, for every table in the schema, and for every column, adding a new column to that table.

  Doubling integrity constraints is more difficult.  The foreign key constraint, for instance, denotes a relationship
  between two tables, a check constraint can contain arbitrary behavior, and a table can only contain one primary key.
  If a schema contains five tables, then at most it can only have five primary keys.  Adding more keys would require
  increasing the number of tables.  An alternative plan could be starting with a very large number of tables with no
  primary keys that could accommodate doubling their number for many trials, but this introduces performance concerns.
  Because of these issues, we focus our attention to constraints that can be doubled as follows: for every table, for
  every constraint, duplicate that constraint and re-add it to the table.  We refer to this doubling strategy as,
  semantic doubling.  Note that constraints doubled this way would not have an impact on what data the schema would
  allow or disallow into a database, since semantic doubling amounts to a restatement of existing constraints.  However,
  since the goal is to evaluate performance, the results should not be affected as long as the test-data generation tool
  still performs work to process the constraints.

  % An example of a general semantic doubling is shown in
  % Figure~\ref{fig:generaldouble}.

  \textbf{Automatic Experiment}To determine worst case complexity, an input
  $n$ is doubled until the ratio $f(2n) / f(n)$ converges to a stable value.
  To account for random error, every time $n$ is doubled, $f(n)$ is recorded
  ten times, and the median time is used for calculating the ratios.  We chose
  median to minimize the effect of outliers. If mean is used instead, a
  single abnormally long run could have a large effect on the result. The
  overall structure of the experiment is shown in Figure~\ref{fig:doublingexp}.

  Convergence checking is necessary because of the fact that worst-case
  time is only apparent for large values of $n$. If too few doubles
  are tried, then the experiment may terminate before $n$ reaches a value
  where the true worst-case time complexity is apparent. At the same time,
  for inefficient  algorithms, each additional doubling run incurs a substantial
  time overhead. For the sake of efficiently, the experiment should
  terminate as quickly as possible.

  To test for convergence, for every time $t$, where $t$ denotes the
  number of times the input has been doubled, we record the each
  doubling ratio $r_t = \frac{f(2^t n)}{f(2^{t-1}n)}$. The current ratio
  $r_c$ is compared to a previous ratio $r_p$ where $p$ is determined by
  a $\mathit{lookback}$ value, such that $p=c-\mathit{lookback}$.  The
  result of the comparison is a $\mathit{difference}$ value, given by
  $\mathit{difference} = |r_c - r_p|$.  This is then compared to a
  tolerance value, and the experiment is judged to have converged when
  $\mathit{difference}<\mathit{tolerance}$.  The $\mathit{lookback}$ and
  $\mathit{tolerance}$ values are both configured before the experiment is
  run.

  Another consequence of worst-case time only being apparent for large
  $n$, is that a very small initial $n$ may appear to converge to one,
  which indicates constant time. To prevent the
  experiment from incorrectly terminating given a small starting $n$, we
  require that a program under study display a ratio of one for a
  $\mathit{minimum}$ number of times before judging that the ratio does in fact converge to one.
  That is, if $r_c = 1$, $t > \mathit{minimum}$ must be true in addition
  to the tolerance test before the experiment is declared convergent.
  The $\mathit{minumum}$ parameter is also configured before an
  experiment.
  Because a doubling ratio of one signifies constant or logarithmic
  time complexity, requiring these doubles does not significantly increase the time needed
  to run the experiment, while providing assurance that a small ratio is not due
  to an insufficiently small $n$.
