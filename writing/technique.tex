%!TEX root=seke.tex
% mainfile: ../seke.tex

\vspace*{-.075in}
\section{Automated Doubling Experiments}\label{sec:technique}
\vspace*{-.075in}

  \begin{figure*}
    \centering
    \input{writing/diagram}
    % \vspace*{-.1in}
    \caption{Technique for conducting automatic doubling experiments.}~\label{fig:doublingexp}
    \vspace*{-.3in}
  \end{figure*}

  % GMK NOTE: We need to make sure that at some point we also talk about the automated analysis of the results.

  {\bf Overview}. The presented technique for performing automatic doubling experiments consists of two key components.
  The first is a method for systematically doubling the initially input relational schema, and the second is a rule for
  determining when a valid conclusion can be drawn from the experiment, thus allowing the doubling process to stop.

  \textbf{Doubling Schemas}. Determining worst-case complexity by a doubling experiment requires that the size of the
  input be doubled. A relational database schema is a complex artifact with many features and interrelationships.  This
  makes doubling rule implementation a non-trivial task.

  % GMK NOTE: This paragraph was too long for the reader to easily understand. Breaking it up!

  A relational database schema contains tables and columns, and constraints that restrict the values allowed into these
  entities. Since the runtime of a schema testing technique may be affected by the number of any of these, it is
  desirable to have a strategy for doubling each one. Doubling the number of tables or columns in a schema is relatively
  easy.  It is possible to double the number of tables in a schema by following this rule: for every table present in
  the schema, create a new empty table. It is important that the new tables be empty to avoid changing more than one
  doubling parameter at once---if the new tables contained columns, for instance, then the number of tables and columns
  in the schema both would be increased, thus interfering with assessing table doubling's impact on
  performance.  Additionally, doubling the number of columns can be accomplished by, for every table in the schema, and
  for every column, adding a new column to that table.

  % GMK NOTE: I think that this point could lead to confusion. Since it is not essential, I am removing it for now.

  % An alternative plan could be starting with a very large number of tables with no primary keys that could accommodate
  % doubling their number for many trials, but this introduces unnecessary performance concerns.

  Doubling integrity constraints is more challenging.  The {\tt FOREIGN KEY} constraint, for instance, denotes a relationship
  between two tables, thus making it difficult to double without introducing extraneous database entities or cyclic
  dependencies.  Since a {\tt CHECK} constraint can express arbitrary conditions, it is also challenging to double if the
  meaning of each constraint must be considered to ensure satisfiability.  Since a table can only contain one {\tt
  PRIMARY KEY}, if a schema contains five tables, then at most it can have five {\tt PRIMARY KEY} constraints, as adding more
  keys would require creating more tables, which should be avoided.

  % GMK QUESTION: So, **what are** the constraints that can be doubled in this fashion? This must be clarified.
  %CBK ANSWER: NOT NULL, UNIQUE, and CHECK

  % GMK NOTE: I recognize that we always used this phrase. But, does it really add that much to the paper?
  % CBK NOTE: No I suppose not

  % We refer to this doubling strategy as, semantic doubling.

  Because of these issues, and others like them, we focus our attention on constraints that can be doubled as follows:
  for every table and for every constraint, duplicate that constraint and re-add it to the table.  Constraints such as
  {\tt NOT NULL}, {\tt UNIQUE}, and {\tt CHECK} are amenable to doubling in this fashion.  It is worth noting that
  constraints doubled this way would not have an impact on what data the schema would allow or disallow into a database,
  since they amount to a restatement of existing constraints.  However, since the goal is to evaluate performance, the
  results should not be affected as long as the test data generation technique must still process and consider these
  additional constraints.

  \textbf{Automatic Experimentation}. To determine worst-case complexity, an input $n$ is doubled until the ratio $f(2n)
  / f(n)$ converges to a stable value.  To account for random error, every time $n$ is doubled, $f(n)$ is computed ten
  times and the median time is used for calculating the ratios; we chose the median to minimize the effect of outliers.
  If the mean is used instead, then a single abnormally long run could have an outsized impact on the result.
  Figure~\ref{fig:doublingexp} shows the overall structure of the experimentation framework.

  Convergence checking is necessary because of the fact that worst-case time is only evident for large values of $n$.
  If too few doubles are tried, then the experiment may terminate before $n$ reaches a value where the true worst-case
  time complexity is apparent. At the same time, for inefficient  algorithms, each additional doubling run incurs a
  substantial time overhead. For the sake of efficiency, the experiment should terminate as quickly as possible.

  % CBK NOTE: Introducing the new equation for ratio with respect to time is helpful for explaining the convergence
  % algorithm, but seems like a restatement of the ratio equation.  I considered only using this new equation when
  % discussing the ratio for consistency, but it is unnecessarily complicated for earlier content

  To test for convergence, for every time $t$, where $t$ denotes the number of times the input has been doubled, we
  record the each doubling ratio $r_t = \frac{f(2^t n)}{f(2^{t-1}n)}$. The current ratio $r_c$ is compared to a previous
  ratio $r_p$ where $p$ is determined by a $\mathit{lookback}$ value, such that $p=c-\mathit{lookback}$.  The result of
  the comparison is a $\mathit{difference}$ value, given by $\mathit{difference} = |r_c - r_p|$.  This is then compared
  to a $\mathit{tolerance}$ value, and the experiment is judged to have converged when $\mathit{difference}<\mathit{tolerance}$.
  The $\mathit{lookback}$ and $\mathit{tolerance}$ values are both configured before the experiment is run.

  Another consequence of worst-case time only being apparent for large $n$, is that a very small initial $n$ may appear
  to converge to one, which indicates constant time complexity. To prevent the experiment from incorrectly terminating
  given a small starting $n$, our method requires that a program under study display a ratio of one for a $\mathit{minimum}$
  number of times before judging that the ratio does in fact converge to one.  That is, if $r_c = 1$, $t >
  \mathit{minimum}$ must be true in addition to the tolerance test before the experiment is declared convergent.  The
  $\mathit{minimum}$ parameter is also configured before an experiment.  Because a doubling ratio of one signifies
  constant or logarithmic time complexity, requiring these doubles does not significantly increase the time needed to
  run the experiment, while providing assurance that a small ratio is not due to an insufficiently small $n$.
