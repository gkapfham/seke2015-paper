%!TEX root=seke.tex
% mainfile: ../seke.tex

\section{Automated Doubling Experiments}\label{sec:technique}

  \begin{figure*}
    \input{writing/diagram}
    \caption{Technique for conducting automatic doubling experiments.}
    \label{fig:doublingexp}
  \end{figure*}

  Our technique for performing automatic doubling experiments consists of two parts.  The first is a method for
  systematically doubling the initial input schema schema, and the second is a rule for determining when a conclusion
  can be drawn from the experiment, and the experiment can be stopped.

  \subsection{Doubling Schemas}
  \label{subsec:doubling}

  Determining worst case complexity by doubling experiment requires that
  the size of the input be doubled. A relational database
  schema is a complex artifact with many features and interrelationships.
  This makes determining meaningful doubling rules a non-trivial task.

  % \begin{figure*}
  %   \centering
  %   \centering
  %   \includegraphics[width=1.25\linewidth]{../diagrams/genDouble}
  %   \caption{Multi Purpose Double Program.}
  %   \label{fig:generaldouble}
  % \end{figure*}

  A relational database schema contains tables, columns, and can contain
  constraints; the runtime of a schema testing tool may be affected by the
  number of any of these. Ideally, we would have a strategy for doubling,
  or a doubler, for each. Doubling the number of tables or columns in a
  schema is relatively easy.  We can double the number of tables in a schema
  by following the rule: for every table present in the schema, we create
  a new empty table. It is important that the new tables be empty to avoid
  changing multiple variables at once.  If the new tables contained
  columns, for instance, then the number of tables and columns in the schema
  would be increased, interfering with our ability to measure the effect of
  tables alone.  Doubling the number of columns can be accomplished by, for
  every table in the schema, for every column, add a new column to that table.
  Doubling integrity constraints is more difficult.  The foreign key
  constraint for instance, donates a relationship between two tables, a check
  constraint can contain arbitrary behavior, and a table can only contain one
  primary key.  If a schema contains five tables, then at most it can only
  have five primary keys.  Adding more keys would require increasing the
  number of tables.  An alternative plan could be starting with a very large
  number of tables with no primary keys that could accommodate doubling their
  number for many trials, but this introduces performance concerns.  Because
  of these issues, we focus our attention to constraints that can be doubled
  as follows: for every table, for every constraint, duplicate that
  constraint and re-add it to the table.  We refer to this doubling strategy
  as, semantic doubling.  Note that constraints doubled this way would not have
  an impact on what data the schema would allow or disallow into a database,
  since semantic doubling amounts to a restatement of existing constraints.
  However, since the goal is to evaluate performance, the results should not be
  affected as long as the test-data generation tool still performs work to
  process the constraints.

  % An example of a general semantic doubling is shown in
  % Figure~\ref{fig:generaldouble}.

  \subsection{Automatic Experiment}
  \label{subsec:experiment}

  To determine worst case complexity, an input $n$ is doubled until the
  ratio $f(2n) / f(n)$ converges to a stable value. To account for random
  error, every time $n$ is doubled, $f(n)$ is recorded ten times, and the
  median time is used for calculating the ratios.  We chose
  median to minimize the effect of outliers. If mean is used instead, a
  single abnormally long run could have a large effect on the result. The
  overall structure of the experiment is shown in Figure~\ref{fig:doublingexp}.

  This convergence checking is necessary because of the fact that worst-case
  time is only apparent for large values of $n$. If too few doubles
  are tried, then the experiment may terminate before $n$ reaches a value
  where the true worst-case time complexity is apparent. At the same time,
  for inefficient  algorithms, each additional doubling run incurs a substantial
  time overhead. For the sake of efficiently, the experiment should
  terminate as quickly as possible.

  To test for convergence, the last four ratios are compared, and the
  sum of differences between them is compared to a tolerance value,
  difference $= |(r_4 - r_3) + (r_3 -r_2) + (r_2 - r_1)|$.


  Another consequence of worst-case time only being apparent for large
  $n$, is that a very small initial $n$ may appear to converge to one,
  which indicates constant time. To prevent the
  experiment from incorrectly terminating given a small starting $n$, we
  require that a program under study display a ratio of one for many
  runs before judging that the ratio does in fact converge to one. Because
  one signifies constant or logarithmic
  time, requiring these doubles does not significantly increase the time needed
  to run the experiment, while providing assurance that a small ratio is not due
  to an insufficiently small $n$.
