%!TEX root=seke.tex
% mainfile: ../seke.tex

\textbf{Results}. Our experiments reveal that for the number of \texttt{Uniques, NotNulls, and Checks},
\textit{SchemaAnalyst} displays linear or lineararithmic worst-case time complexity.  Out of the 699 experiments
performed doubling these schema structures, $72\%$ converged to linear or linearithmic.  Another $8\%$ failed to
converge, and of these experiments, $80\%$ failed because of memory limitations, $13\%$ exceeded the maximum time limit,
and for $8\%$ the reason for failure could not be determined.  The doubling ratios among these experiments were
primarily linear or linerarithmic at the time they were terminated, however there were some that were quadratic and
three that were cubic.  The experiments that failed to converge were primarily complicated schemas such as iTrust and
BioSQL, and criterions at the top of the hierarchy, such as ANCC and AUCC. The remaining $20\%$ converged on constant or
logarithmic.  Since there did not seem to be a pattern in which configurations converged this way compared to linear or
linerarithmic, it is likely that these experiments terminated before the true worst-case time complexity was apparent.

For the schema structures tables and columns, the results were less conclusive. Doubling the number of tables in the
schema caused the runtime of \textit{SchemaAnalyst} to increase much faster than the integrity constraints. As a result,
$56\%$ of the 467 experiments doubling this schema feature were terminated after exceeding the experiment time limit
before they converged.  Of the experiments that converged, 72 converged to quadratic, and 10 converged to cubic.  Of the
experiments that terminated before they converged, the doubling ratios for 205 indicated quadratic, 18 indicated cubic,
and 37 were worse than cubic.

Experiments on the number of columns were also inconclusive.  208 experiments that converged showed linear or
linearitmic time complexity, while 28 converged to quadratic and 2 cubic.  Another 203 experiments failed to converge,
however unlike the experiments for the number of tables, the experiments for the number of columns most frequently
failed by running out of memory rather than running out of time. The experiments that did not converge include 106
ratios indicating quadratic behavior, 73 cubic, and 3 worse.

To gain a more nuanced understanding of the results, we construct a regression tree predicting runtime by the predictors
\texttt{Tables, Columns, Uniques, NotNulls, Checks, Criterion, and DataGenerator}. The package \textit{ctree} for the R
language was used to produce the tree, shown as Figure~\ref{fig:atree}. The regression tree confirms that the number of
tables has the largest impact on runtime, as can be seen by the fact that the node 1 splits on the number of tables, and
the significant difference between nodes 6 and 7, which are also distinguished by the number of tables according to node
5. The tree also reveals that when the number of tables in the schema is small, the choice of coverage criterion is the
most important predictor for runtime.  This is shown by node 2, however, the nodes resulting from this prediction, nodes
3 and 4, do not seem very distinct.

To gain more insight into the behavior of \textit{SchemaAnalyst} when the number of tables is small, a new tree was
constructed with the same parameters, with the exception that \texttt{Tables} was removed from the list of predictors.
The resulting regression tree is shown as Figure~\ref{fig:ttree}.  Node 1 in the new tree also indicates that the choice
of criterion has the largest impact when the number of tables is not considered.  Node 2 shows that the next most
significant predictor is the data generator, and node 3 shows the next most significant factor is the number of columns
in the schema.  The differences between the leaves of the tree however, are still not readily apparent.

According to the decisions produced by the regression tree, the choice of coverage criterion and data generator have an
impact on the runtime of \textit{SchemaAnalyst}. To show the effect of these choices, we present
Figure~\ref{fig:crites}, which shows the effect of coverage criterion on runtime, and Figure~\ref{fig:datas}, which
shows the effect of data generator on runtime.

For coverage criteria, the most apparent pattern is that AUCC, ClauseAICC, and CondAICC seem to cause runtime to
increase by about the same amount, with the other criterions taking roughly the same amount of time.

For data generator, the Random and Random Defaults generators took the most amount of time by a distinctive margin, and
a less pronounced hierarchy between AVM, AVM defaults, Directed Random, and Directed Random Defaults can be observed.

While the box and whisker plots allow us to see how choices between coverage criterions and data generators affects
runtime, the question remains if these differences are statistically and practically significant. To answer this
question, we calculate the Wilcoxon rank sum test and the $\hat{A}_{12}$ test.

We perform these tests for every pair of coverage criterions, and every pair of data generators.  Table~\ref{tab:crites}
shows the results of these tests for coverage criteria, and Table~\ref{tab:datas} shows the results for data generators.

The Wilcoxon rank sum test is a nonparematric test for hypothesis
testing.  If the result of the test is greater than the significance
level ($0.05$ is frequently used), then the collections are
indistingushable.  If however, then result is less, then the collections
are differentiable.  The $\hat{A}_{12}$ test is similar, but for drawing
conclusions about the pratical differance between two collections of
data.  The rank sum test reveals if the collections are statistically differant, while
the $\hat{A}_{12}$ test determines if the collections are differant
enough to have a noticeable impact in pratice.



\begin{figure*}
\centering
  \centering
  \includegraphics[width=.75\linewidth]{diagrams/AllTree.pdf}
  \caption{Regression tree using all variables to predict runtime in
  minutes. \vspace{-.15in}}
  \label{fig:atree}
  \vspace{-.15in}
\end{figure*}

\begin{figure*}
\centering
  \centering
  \includegraphics[width=.75\linewidth]{diagrams/NoTableCtreesd.pdf}
  \caption{Regression tree predicting runtime excluding Tables.\vspace{-.15in}}
  \label{fig:ttree}
  \vspace{-.15in}
\end{figure*}


\begin{figure}
\centering
  \centering
  \includegraphics[width=1\linewidth]{diagrams/CriterionvsTime.pdf}
  \caption{Coverage criterion versus runtime in minutes.\vspace{-.15in}}
  \label{fig:crites}
  \vspace{-.15in}
\end{figure}

\begin{figure}
\centering
  \centering
  \includegraphics[width=1\linewidth]{diagrams/DataGeneratorvsTime.pdf}
  \caption{Data generator versus runtime in minutes.\vspace{-.15in}}
  \label{fig:datas}
  \vspace{-.15in}
\end{figure}


\begin{table*}[h]
\begin{tabular}{llllllllll}
           & APC      & ANCC     & CondAICC & NCC      & AUCC     & AICC     & ClauseAICC & ICC      & UCC   \\
APC        & NA       & \cellcolor{gray!25}0.425    &
\cellcolor{gray!45}0.337    & 0.484    &\cellcolor{gray!45} 0.334    &
\cellcolor{gray!25}0.413    &\cellcolor{gray!45} 0.329      & 0.481    & 0.449 \\
ANCC       & 2.20E-16 & NA       &\cellcolor{gray!25} 0.407
&\cellcolor{gray!25} 0.561    &\cellcolor{gray!25} 0.405    & 0.484
&\cellcolor{gray!25} 0.399      & 0.554    & 0.526 \\
CondAICC   & 2.20E-16 & 2.20E-16 & NA       &\cellcolor{gray!45} 0.671
& 0.503    &\cellcolor{gray!25} 0.581    & 0.492
&\cellcolor{gray!45} 0.656    &\cellcolor{gray!25} 0.634 \\
NCC        & 1.20E-02 & 2.20E-16 & 2.20E-16 & NA
&\cellcolor{gray!45} 0.335    &\cellcolor{gray!25} 0.417
&\cellcolor{gray!45} 0.322      & 0.491    & 0.461 \\
AUCC       & 2.20E-16 & 2.20E-16 & \cellcolor{gray!65}6.92E-01 & 2.20E-16 & NA       &
\cellcolor{gray!25}0.577    & 0.490      &\cellcolor{gray!45} 0.651
&\cellcolor{gray!25} 0.628 \\
AICC       & 2.20E-16 & 1.70E-02 & 2.20E-16 & 2.20E-16 & 2.20E-16 & NA
&\cellcolor{gray!25} 0.412      &\cellcolor{gray!25} 0.571    & 0.547 \\
ClauseAICC & 2.20E-16 & 2.20E-16 & \cellcolor{gray!65}2.72E-01 & 2.20E-16 & 1.40E-01 &
2.20E-16 & NA         & \cellcolor{gray!45}0.662    &\cellcolor{gray!45} 0.641 \\
ICC        & 4.00E-03 & 2.20E-16 & 2.20E-16 &\cellcolor{gray!65} 1.83E-01 & 2.20E-16 & 2.20E-16 & 2.20E-16   & NA       & 0.472 \\
UCC        & 9.30E-16 & 3.83E-05 & 2.20E-16 & 7.36E-10 & 2.20E-16 & 5.73E-13 & 2.20E-16   & 9.29E-06 & NA    \\
\end{tabular}
\caption{For each pair of coverage criterions, lower left shows Wilcox
Rank Sum Test, upper right shows $\hat{A}_{12}$.}
\label{tab:crites}
\end{table*}


\begin{table*}[h]
\begin{tabular}{lllllll}
                         & Random   & Random Defaults & Directed Random & Directed Random Defaults & AVM      & AVM Defaults \\
Random                   & NA       & 0.538
&\cellcolor{gray!65} 0.740           &\cellcolor{gray!65} 0.789
&\cellcolor{gray!25} 0.627    &\cellcolor{gray!45} 0.680        \\
Random Defaults          & 6.59E-13 & NA              &
\cellcolor{gray!45}0.673           &\cellcolor{gray!45} 0.701
&\cellcolor{gray!25} 0.564    & \cellcolor{gray!25}0.617        \\
Directed Random          & 2.20E-16 & 2.20E-16        & NA
& 0.543                    &\cellcolor{gray!25} 0.360    &\cellcolor{gray!25} 0.435        \\
Directed Random Defaults & 2.20E-16 & 2.20E-16        & 9.74E-16
& NA                       &\cellcolor{gray!45} 0.328    &\cellcolor{gray!25} 0.395        \\
AVM                      & 2.20E-16 & 2.20E-16        & 2.20E-16
& 2.20E-16                 & NA       &\cellcolor{gray!25} 0.572        \\
AVM Defaults             & 2.20E-16 & 2.20E-16        & 2.20E-16        & 2.20E-16                 & 2.20E-16 & NA
\end{tabular}
\caption{For each pair of Data Generators, lower left shows Wilcox Rank
Sum Test, upper right shows $\hat{A}_{12}$.}
\label{tab:datas}
\end{table*}


\textbf{Threats to Validity}. Our technique for doubling the number of constraints on the schema is simply to
duplicate the existing constraints. It is possible that \textit{SchemaAnalyst} does less work processing these
% CBK Note: 'unique' is a poor word choice since it's a constraint name
redundant constraints than it would given unique constraints. However, doubling the check constraints in this
way is an easy to implement, semantically significant way of evaluating \textit{SchemaAnalyst}.

Additionally, since worst-case time is only apparent for large $n$, it
is possible that the experiment terminated too quicly.  While we
attempted to configure the parameters of our tool using algorithms with
known worst-case complexities and conducting preliminary experiments with
various settings under manual supervision, it is possible that our
configuration was not optimized for use on the HPC cluster. 

